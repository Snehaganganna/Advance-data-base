{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af2bcea-2bbe-40bf-918e-df698e978bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project 1\n",
    "### Sneha Ganganna (5579362)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d69a15-0a90-4f2a-a7a3-50c8be9b215b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark-3.4.0-bin-hadoop3/python (3.4.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "#To check if the spark has been installed \n",
    "!pip install pyspark\n",
    "import pyspark\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from collections import Counter\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import countDistinct\n",
    "#from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, udf,explode, split, desc\n",
    "from pyspark.sql.types import StringType\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b96ae8e-6e34-43d9-a707-b82910aa0239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Spark Session initialisation\n",
    "sparkSession = SparkSession.builder.appName(\"Project 1\")\\\n",
    "    .config(\"spark.memory.fraction\", 0.8) \\\n",
    "    .config(\"spark.driver.cores\", 8) \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\" , \"1000\") \\\n",
    "    .getOrCreate()\n",
    "sc = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b6697-266b-495c-b5c2-888790d09e11",
   "metadata": {},
   "source": [
    "#Exercise 1. 2 (Loading the dataset into an RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a01d2a6-fb14-45b3-bfad-0fd18f6c2281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read users_libraries file into RDD\n",
    "userRatingsRDD = sc.textFile(\"users_libraries\")\n",
    "# Create Pair RDD for users_libraries Text File\n",
    "userRatingsRDD = sc.textFile(\"users_libraries.txt\").map(lambda line: line.split(\";\")).map(lambda line: (line[0],list(map(int,line[1].split(\",\")))))\n",
    "\n",
    "#PaperCsv RDD\n",
    "def processCsvFile(line):\n",
    "    paper = csv.reader([line.replace(\"\\0\", \"\")], delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    paperList = next(paper)\n",
    "    return paperList[0], paperList[14]\n",
    "\n",
    "paperTermsRDD = sc.textFile(\"papers.csv\")\n",
    "paperTermsRDD = paperTermsRDD.map(processCsvFile).filter(lambda x: (x[1] != \"\" and x[1] != \" \")).map(lambda x: (int(x[0]),x[1].split(\" \")))\n",
    "\n",
    "# Join the two RDDs on the common key i.e. paper_id\n",
    "#userLibrariesJoinPapersRdd = userRatingsRDD.flatMapValues(lambda x: x).map(lambda x: (x[1],x[0])).join(paperTermsRDD)\n",
    "\n",
    "# Print the result\n",
    "#paperTerms_userRatings.foreach(print)\n",
    "#paperTerms_userRatings.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09877fec-ca91-4300-9b3a-761f8b992d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) to create RDD's: 0.40788912773132324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#to record execution time\n",
    "start_time = time.time()\n",
    "\n",
    "#load stopwords_en file\n",
    "stopWords = sc.textFile(\"stopwords_en.txt\")\n",
    "stopWordsBroadcast = sc.broadcast(stopWords.collect())\n",
    "\n",
    "#print(\"Stop words Start...\")\n",
    "#stopWordsBroadcast.value\n",
    "#print(\"Stop words End...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) to create RDD's:\", end_time-start_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be2f45-b623-4785-865a-c8545f2f6a57",
   "metadata": {},
   "source": [
    "#Exercise 1. 3 (Joining collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7421f21b-4ad4-4d7f-9589-8d3aa650ce16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o111.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 1 times, most recent failure: Lost task 3.0 in stage 4.0 (TID 21) (6c091ee0c4a7 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3389, in func\n    for x in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 4239, in map_values_fn\n    return kv[0], f(kv[1])\n  File \"/tmp/ipykernel_14143/1817692000.py\", line 20, in removeStopWords\nNameError: name 'a' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2316)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3389, in func\n    for x in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 4239, in map_values_fn\n    return kv[0], f(kv[1])\n  File \"/tmp/ipykernel_14143/1817692000.py\", line 20, in removeStopWords\nNameError: name 'a' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 10 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Data\n\u001b[1;32m     41\u001b[0m frequentWordListFile \u001b[38;5;241m=\u001b[39m frequentWordList\u001b[38;5;241m.\u001b[39mmap(CreateCsv)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mfrequentWordListFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution Time taken for Joining collections and writing it to a file:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end_time\u001b[38;5;241m-\u001b[39mstart_time, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:3406\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3404\u001b[0m     keyed\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mBytesToString())\u001b[38;5;241m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   3405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3406\u001b[0m     \u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o111.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 4.0 failed 1 times, most recent failure: Lost task 3.0 in stage 4.0 (TID 21) (6c091ee0c4a7 executor driver): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3389, in func\n    for x in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 4239, in map_values_fn\n    return kv[0], f(kv[1])\n  File \"/tmp/ipykernel_14143/1817692000.py\", line 20, in removeStopWords\nNameError: name 'a' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 10 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2316)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:163)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 3389, in func\n    for x in iterator:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 4239, in map_values_fn\n    return kv[0], f(kv[1])\n  File \"/tmp/ipykernel_14143/1817692000.py\", line 20, in removeStopWords\nNameError: name 'a' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$executeTask$1(SparkHadoopWriter.scala:136)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:135)\n\t... 10 more\n"
     ]
    }
   ],
   "source": [
    "#to record execution time\n",
    "start_time = time.time()\n",
    "\n",
    "#load stopwords_en file\n",
    "stopWords = sc.textFile(\"stopwords_en.txt\")\n",
    "stopWordsBroadcast = sc.broadcast(stopWords.collect())\n",
    "\n",
    "# Finding top 10 most frequent words\n",
    "def FrequentWords(x):\n",
    "    rList = Counter(x)\n",
    "    FrequentWordsCount = CounterList.most_common(10)\n",
    "    FrequentWords = [word for word, word_count in FrequentWordsCount]\n",
    "    return FrequentWords\n",
    "\n",
    "#to Remove the stopWords\n",
    "def removeStopWords(List):\n",
    "    requiredList = List.copy()\n",
    "    for w in List:\n",
    "        if ((w in stopWordsBroadcast.value) or a == \"\" or a == \" \"):\n",
    "            requiredList.remove(a)\n",
    "    return requiredList\n",
    "\n",
    "\n",
    "# join the RDDs\n",
    "userLibrariesJoinPapersRdd = userRatingsRDD.flatMapValues(lambda x: x).map(lambda x: (x[1],x[0])).join(paperTermsRDD)\n",
    "#restructure the RDD\n",
    "userLibrariesJoinPapersRdd = userLibrariesJoinPapersRdd.map(lambda x: (x[1][0],x[1][1]))\n",
    "#transformation of RDD to perform computations \n",
    "userLibrariesJoinPapersRdd = userLibrariesJoinPapersRdd.flatMapValues(lambda x:x).groupByKey().mapValues(list)\n",
    "#to remove the stop words from the RDD\n",
    "userLibrariesJoinPapersRddWithoutStopWords = userLibrariesJoinPapersRdd.mapValues(removeStopWords)\n",
    "\n",
    "# now run the def frequentWords after the stop words are removed\n",
    "frequentWordList = userLibrariesJoinPapersRddWithoutStopWords.mapValues(FrequentWords)\n",
    "\n",
    "#wrtiting the above data into file\n",
    "def CreateCsv(data):\n",
    "    Data = data[0] + \",\" + (','.join(str(d) for d in data[1]))\n",
    "    return Data\n",
    "\n",
    "frequentWordListFile = frequentWordList.map(CreateCsv)\n",
    "frequentWordListFile.saveAsTextFile(\"Results\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time taken for Joining collections and writing it to a file:\", end_time-start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb339fa5-aafd-41ae-bdb5-d5540eda9649",
   "metadata": {},
   "source": [
    "#Exercise 1. 4 (Basic Analysis for Recommender Systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f254ed-b419-4d72-913a-40e942ae1927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct users:  28416\n",
      "Number of distinct items:  172079\n",
      "Number of ratings:  828481\n",
      "Minimum number of ratings a user has given:  1\n",
      "Maximum number of ratings a user has given:  1922\n",
      "Average number of ratings a user has given:  29.155440596846848\n",
      "Standard deviation of ratings for users 81.1751761366871\n",
      "Minimum number of ratings an item has received:  3\n",
      "Maximum number of ratings an item has received:  924\n",
      "Average number of ratings per item:  4.81453867119172\n",
      "Standard deviation of ratings per item:  5.477802292314525\n",
      "Execution Time taken for Basic Analysis for Recommender Systems: 5.430299282073975 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#to record execution time\n",
    "start_time = time.time()\n",
    "\n",
    "#a) Number of (distinct) user, number of (distinct) items, and number of ratings\n",
    "\n",
    "#first apply flatMapVales() function for each kay-pair value in userRatingsRDD\n",
    "userRatingsRDDFMV =userRatingsRDD.flatMapValues(lambda x:x)\n",
    "\n",
    "numOfUsers = userRatingsRDDFMV.keys().distinct().count()\n",
    "print(\"Number of distinct users: \", numOfUsers)\n",
    "\n",
    "numOfItems = userRatingsRDDFMV.values().distinct().count()\n",
    "print(\"Number of distinct items: \", numOfItems)\n",
    "\n",
    "numOfRatings = userRatingsRDDFMV.count()\n",
    "print(\"Number of ratings: \", numOfRatings)\n",
    "\n",
    "#b) Min number of ratings a user has given\n",
    "minNumberOfRatings = userRatingsRDD.map(lambda x: (x[0],len(x[1]))).map(lambda x: x[1]).min()\n",
    "print(\"Minimum number of ratings a user has given: \", minNumberOfRatings)\n",
    "\n",
    "#c)  Max number of ratings a user has given\n",
    "maxNumberOfRatings = userRatingsRDD.map(lambda x: (x[0],len(x[1]))).map(lambda x: x[1]).max()\n",
    "print(\"Maximum number of ratings a user has given: \", maxNumberOfRatings)\n",
    "\n",
    "#d)  Average number of ratings of users\n",
    "avgNumberOfRatings = numOfRatings/numOfUsers\n",
    "print(\"Average number of ratings a user has given: \", avgNumberOfRatings)\n",
    "\n",
    "#e) Standard deviation for ratings of users\n",
    "stdevRatingsOfUsers = (userRatingsRDD.map(lambda x: (x[0],len(x[1]))).map(lambda x: x[1])).stdev()\n",
    "print(\"Standard deviation of ratings for users\",stdevRatingsOfUsers)\n",
    " \n",
    "#f)Min number of ratings an item has received\n",
    "minRatingsOfitem = userRatingsRDDFMV.map(lambda x: (x[1],1)).reduceByKey(lambda x,y: x+y).map(lambda x: x[1]).min()\n",
    "print(\"Minimum number of ratings an item has received: \", minRatingsOfitem)\n",
    "\n",
    "#g) Max number of ratings an item has received\n",
    "maxRatingsOfitem = userRatingsRDDFMV.map(lambda x: (x[1],1)).reduceByKey(lambda x,y: x+y).map(lambda x: x[1]).max()\n",
    "print(\"Maximum number of ratings an item has received: \", maxRatingsOfitem)\n",
    "\n",
    "#h) Average number of ratings of items\n",
    "avgRatingsOfitem= numOfRatings/numOfItems\n",
    "print(\"Average number of ratings per item: \", avgRatingsOfitem)\n",
    "\n",
    "#i) Standard deviation for ratings of items\n",
    "stdevRatingsOfItems = userRatingsRDDFMV.map(lambda x: (x[1],1)).reduceByKey(lambda x,y: x+y).map(lambda x: x[1]).stdev()\n",
    "print(\"Standard deviation of ratings per item: \", stdevRatingsOfItems)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time taken for Basic Analysis for Recommender Systems:\", end_time-start_time,\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47a9f3-5774-4c3f-95cb-01f49ef50828",
   "metadata": {},
   "source": [
    "#Exercise 1. 5 (Loading the dataset into Data Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75cabce-a1d5-4e98-8d1e-6bdcfe5473de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) to create Dataframes: 8.308101654052734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# To define the schema of the CSV data\n",
    "papersSchema = StructType([\n",
    "    StructField(\"paper_id\", IntegerType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"bookـtitle\", StringType(), True),\n",
    "    StructField(\"series\", IntegerType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"pages\", IntegerType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"postedat\", FloatType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True)\n",
    "    ])\n",
    "df_papers = sparkSession.read.csv(\"papers.csv\", sep = \",\", header = True, schema = papersSchema)\n",
    "df_papers = df_papers.na.drop(subset=[\"abstract\"]) # drops the null values\n",
    "\n",
    "# To define the schema of the text data\n",
    "userLibrariesSchema = StructType([\n",
    "    StructField(\"user_hash_id\",StringType(),True),\n",
    "    StructField(\"user_library\",StringType(),True)\n",
    "])\n",
    "df_userLibraries = sparkSession.read.csv(\"users_libraries.txt\", sep = \";\", header = True, schema = userLibrariesSchema)\n",
    "# Select the columns using selectExpr() function\n",
    "df_userLibraries = df_userLibraries.selectExpr(\"user_hash_id\",\"split(user_library,',') AS user_library\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_userLibraries.head()\n",
    "\n",
    "df_papers.head()\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) to create Dataframes:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca82f4-1ecc-47b7-8195-20f63cd27cb4",
   "metadata": {},
   "source": [
    "#Ex1.6 - Using Dataframes for Ex1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0739b71d-aff3-4b9a-99b0-0a5a047b861e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time taken(in seconds) for Joining collections and writing to file 186.3451726436615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#Explode UserLibrary\n",
    "userLibraryExplode = df_userLibraries.select(df_userLibraries.user_hash_id,explode(df_userLibraries.user_library).alias(\"paper_id\"))\n",
    "\n",
    "#Join UserLibrary and PaperCsv\n",
    "df_userLibraryJoinPaperCsv = df_papers.join(userLibraryExplode,df_papers.paper_id == userLibraryExplode.paper_id, how=\"inner\").select(userLibraryExplode.user_hash_id,userLibraryExplode.paper_id,df_papers.abstract)\n",
    "\n",
    "\n",
    "# Define the useless words list\n",
    "useless_words = ['',' ','\"']\n",
    "\n",
    "# Filter out rows with empty or useless abstracts\n",
    "df_userLibraryJoinPaperCsv = df_userLibraryJoinPaperCsv.filter(\n",
    "    (~col(\"abstract\").isin(useless_words)) &\n",
    "    (col(\"abstract\").isNotNull())\n",
    ")\n",
    "\n",
    "# Define a UDF to remove stop words from the abstract\n",
    "def remove_stop_words(abstract):\n",
    "    words = abstract.split(\" \")\n",
    "    cleaned_words = [word for word in words if word.lower() not in stopWordsBroadcast.value]\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "# Register the UDF with Spark\n",
    "remove_stop_words_udf = udf(remove_stop_words, StringType())\n",
    "\n",
    "# Apply the UDF to the \"abstract\" column\n",
    "df_userLibraryJoinPaperCsvWithoutStopWords = df_userLibraryJoinPaperCsv.withColumn(\"abstract\", remove_stop_words_udf(col(\"abstract\")))\n",
    "\n",
    "# Now df_userLibraryJoinPaperCsvWithoutStopWords contains the DataFrame with stop words removed from the \"abstract\" column.\n",
    "\n",
    "\n",
    "#Finding top 10 most frequent words\n",
    "df_userLibraryJoinPaperCsvWithoutStopWordsCount = df_userLibraryJoinPaperCsvWithoutStopWords.groupBy(\"user_hash_id\",\"abstract\").count().withColumnRenamed(\"count\", \"word_count\")\n",
    "userWords_window = Window.partitionBy(df_userLibraryJoinPaperCsvWithoutStopWordsCount.user_hash_id).orderBy(col(\"word_count\").desc())\n",
    "df_userLibraryJoinPaperCsvWithoutStopWordsRank = df_userLibraryJoinPaperCsvWithoutStopWordsCount.withColumn(\"word_rank\",rank().over(userWords_window))\n",
    "df_topFrequentWordsPerUser = df_userLibraryJoinPaperCsvWithoutStopWordsRank.filter(df_userLibraryJoinPaperCsvWithoutStopWordsRank[\"word_rank\"]<11)\n",
    "df_groupedTop10FrequentWordsPerUser = df_topFrequentWordsPerUser.groupBy(\"user_hash_id\").agg(collect_list(\"abstract\")).withColumnRenamed(\"collect_list(abstract)\", \"abstract_word_list\")\n",
    "\n",
    "#writing top 10 most frequent words of each user to file\n",
    "df_groupedTop10FrequentWordsPerUser.write.save(\"Outputs/Top10WordsForEachUser_DF\")\n",
    "\n",
    "#print(\"Top 10 most frequent words of each user_hash_id Start...\")\n",
    "#df_groupedTop10FrequentWordsPerUser.take(10)\n",
    "#print(\"Top 10 most frequent words of each user_hash_id End...\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) for Joining collections and writing to file\", end_time-start_time)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb12738-202b-49e2-a500-29023d579066",
   "metadata": {},
   "source": [
    "#Ex1.6 - Using Dataframes for Ex1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a02ad0-72b9-45b1-a5ec-72d15ed18d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (distinct) user: 28415\n",
      "Number of (distinct) items: 172079\n",
      "Number of ratings: 828461\n",
      "Min number of ratings a user has given: 1\n",
      "Max number of ratings a user has given: 1922\n",
      "Average number of ratings of users: 29.15576280133732\n",
      "Standard deviation for ratings of users: 81.17801478819581\n",
      "Min number of ratings an item has received: 2\n",
      "Max number of ratings an item has received: 924\n",
      "Average number of ratings of items: 4.814422445504681\n",
      "Standard deviation for ratings of items: 5.47783230760693\n",
      "Execution Time taken(in seconds) for Basic Analysis for Recommender Systems: 195.0146381855011\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, countDistinct, avg, stddev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a\n",
    "noOfDistinctUsers_df = userLibraryExplode.select(countDistinct(\"user_hash_id\")).collect()[0][0]\n",
    "noOfDistinctItems_df = userLibraryExplode.select(countDistinct(\"paper_id\")).collect()[0][0]\n",
    "noOfRatings_df = userLibraryExplode.count()\n",
    "\n",
    "# b, c, d, e\n",
    "ratingsList_df = userLibraryExplode.groupBy(\"user_hash_id\").count().withColumnRenamed(\"count\", \"no_of_items\")\n",
    "ratingsList_df = ratingsList_df.describe(\"no_of_items\")\n",
    "\n",
    "summary_df = ratingsList_df.select(\"summary\", \"no_of_items\").rdd.collectAsMap()\n",
    "\n",
    "minNoOfRatingUserHasGiven = summary_df[\"min\"]\n",
    "maxNoOfRatingUserHasGiven = summary_df[\"max\"]\n",
    "avgNumberOfRatingUserGave = noOfRatings_df / noOfDistinctUsers_df\n",
    "standardDeviationOfRating = summary_df[\"stddev\"]\n",
    "\n",
    "# f, g, h, i\n",
    "ratingsListByPaperId_df = userLibraryExplode.groupBy(\"paper_id\").count().withColumnRenamed(\"count\", \"no_of_ratings\")\n",
    "ratingsListByPaperId_df = ratingsListByPaperId_df.describe(\"no_of_ratings\")\n",
    "\n",
    "summary_df = ratingsListByPaperId_df.select(\"summary\", \"no_of_ratings\").rdd.collectAsMap()\n",
    "\n",
    "minNoOfRatingItemHasReceived_df = summary_df[\"min\"]\n",
    "maxNoOfRatingItemHasReceived_df = summary_df[\"max\"]\n",
    "avgNumberOfRatingOfItems_df = noOfRatings_df / noOfDistinctItems_df\n",
    "standardDeviationOfRItem_df = summary_df[\"stddev\"]\n",
    "\n",
    "# Stop the Spark session\n",
    "#spark.stop()\n",
    "\n",
    "\n",
    "print(\"Number of (distinct) user:\" ,noOfDistinctUsers_df)\n",
    "print(\"Number of (distinct) items:\" ,noOfDistinctItems_df)\n",
    "print(\"Number of ratings:\" ,noOfRatings_df)\n",
    "print(\"Min number of ratings a user has given:\",minNoOfRatingUserHasGiven)\n",
    "print(\"Max number of ratings a user has given:\",maxNoOfRatingUserHasGiven)\n",
    "print(\"Average number of ratings of users:\",avgNumberOfRatingUserGave)\n",
    "print(\"Standard deviation for ratings of users:\",standardDeviationOfRating)\n",
    "print(\"Min number of ratings an item has received:\",minNoOfRatingItemHasReceived_df)\n",
    "print(\"Max number of ratings an item has received:\",maxNoOfRatingItemHasReceived_df)\n",
    "print(\"Average number of ratings of items:\",avgNumberOfRatingOfItems_df)\n",
    "print(\"Standard deviation for ratings of items:\",standardDeviationOfRItem_df)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Execution Time taken(in seconds) for Basic Analysis for Recommender Systems:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8b381-1c4d-4115-bd38-866b0b102166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
