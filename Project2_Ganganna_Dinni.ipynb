{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045bd061-133a-4a77-acf4-46d85eb7aa90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project 2\n",
    "### Sneha Ganganna (5579362) Aishwarya Dinni (5653414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adb12f1-a85d-4294-9d2c-4f144d297f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import time\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed277b-545e-448b-9b8d-f79ee22082c0",
   "metadata": {},
   "source": [
    "a)The first task is to pre-process the data. It is required to partition the triples into relations by using vertically partitioned approach, namely for each distinct property, set up a table with ’Subject’ and ’Object’ as columns. Assume there are n properties in the triple store, then you need to construct n tables. One optional step before the pre-processing, is to build up a dictionary of all strings occurring in the triple store and transform the string values into integers. Since the comparison of integer is much faster than the comparison on string values, this optional step helps improve the efficiency of the join algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e557c0-8eda-4a03-a82e-53cef4a6de89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to fetch subject,object,property\n",
    "def pre_process_data(df):\n",
    "    \n",
    "    df[0] = df[0].split(\":\")[1].strip()\n",
    "    df[1] = df[1].split(\":\")[1].strip()\n",
    "    df[2] = df[2].split(\":\")[1][:-1].strip() if len(df[2].split(\":\")) > 1 else df[2][:-1].strip()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6cc870-3094-490a-a529-82438f6b5802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read 100k.txt into a pandas dataframe\n",
    "rawDataDf = pd.read_csv(\"100k.txt\", \n",
    "                     header = None, \n",
    "                     sep=\"\\t\", \n",
    "                     quotechar='\"', \n",
    "                     skipinitialspace=True, \n",
    "                     names=['subject','property','object'])\n",
    "\n",
    "rawDataDf = rawDataDf.apply(pre_process_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b2847e-ceca-4537-9c23-c4553c8a5475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject       property     object\n",
      "0   City0  parentCountry  Country20\n",
      "1   City1  parentCountry   Country0\n",
      "2   City2  parentCountry   Country1\n",
      "3   City3  parentCountry   Country6\n",
      "4   City4  parentCountry  Country15\n"
     ]
    }
   ],
   "source": [
    "print(rawDataDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecab283a-bc45-4bf6-9374-03a918777309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the DataFrame raw_df by the values in the \"property\" column, creating separate groups for each unique property value.\n",
    "dictionaryDf = dict(tuple(rawDataDf.groupby('property', as_index=False)))\n",
    "\n",
    "\n",
    "# Remove the \"property\" column from every DataFrame in the dictionary\n",
    "dictionaryDf = {key: df.drop(\"property\", axis=1).reset_index(drop=True) for key, df in dictionaryDf.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02733d7d-eccd-4b33-8720-ea9a68d98600",
   "metadata": {},
   "source": [
    "## Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a2e2ca0-da96-4c90-89f7-c7315f01648a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sortMergeJoin(leftDf, leftKey, rightDf, rightKey):\n",
    "    \n",
    "    # Perform Sorting based on keys for both DataFrames\n",
    "    sortedLeftDf = leftDf.sort_values(by=leftKey, ascending=True)\n",
    "    sortedRightDf = rightDf.sort_values(by=rightKey, ascending=True)\n",
    "    \n",
    "    # Merge the sorted DataFrames based on the specified keys using an inner join\n",
    "    # The 'pd.merge' operation combines the DataFrames using the common 'leftKey' and 'rightKey' columns.\n",
    "    # The 'how='inner'' parameter ensures that only common rows are included in the result.\n",
    "    resultsDf = pd.merge(sortedLeftDf, sortedRightDf, left_on=leftKey, right_on=rightKey, how='inner')\n",
    "   \n",
    "    # The merged DataFrame containing the common rows based on the specified keys\n",
    "    return resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e277f2-3f02-47c7-908e-ec144ec69c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8680/1620875723.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sortMergeJoin_QueryResult_df.rename(columns = {'subject_x_x':'follows.subject',\n"
     ]
    }
   ],
   "source": [
    "# Sort Merge Join Results\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Sort Merge Join between follows and friendOf DataFrames\n",
    "# followsObjectect_friendOfSubjectect sort merge join Df\n",
    "followsObject_friendOfSubject_smj_df = sortMergeJoin(\n",
    "    dictionaryDf[\"follows\"],\"object\",\n",
    "    dictionaryDf[\"friendOf\"],\"subject\")\n",
    "\n",
    "# Sort Merge Join between likes and hasReview DataFrames\n",
    "# likesObject_hasReviewSubject_sort merge join Df\n",
    "likesObject_hasReviewSubject_smj_df = sortMergeJoin(\n",
    "    dictionaryDf[\"likes\"],\"object\",\n",
    "    dictionaryDf[\"hasReview\"],\"subject\")\n",
    "\n",
    "# Sort Merge Join between the results of the previous two joins\n",
    "# friendsOfObject_likesSubject_sort merge join Df\n",
    "friendsOfObject_likesSubject_smj_df = sortMergeJoin(\n",
    "    followsObject_friendOfSubject_smj_df,\"object_y\",\n",
    "    likesObject_hasReviewSubject_smj_df,\"subject_x\")\n",
    "\n",
    "# Extract only the required columns and rename them for clarity\n",
    "# resultingDf= sortMergeJoin_QueryResult df\n",
    "sortMergeJoin_QueryResult_df = friendsOfObject_likesSubject_smj_df[[\"subject_x_x\",\n",
    "                                     \"object_x_x\",\n",
    "                                     \"object_y_x\",\n",
    "                                     \"object_x_y\",\n",
    "                                     \"object_y_y\"]]\n",
    "\n",
    "sortMergeJoin_QueryResult_df.rename(columns = {'subject_x_x':'follows.subject',\n",
    "                                       'object_x_x':'follows.object',\n",
    "                                       'object_y_x':'friendOf.object',\n",
    "                                       'object_x_y':'likes.object',\n",
    "                                       'object_y_y':' hasReview.object'}, inplace = True)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413feb95-67ae-4c1e-b923-ce92b658260b",
   "metadata": {},
   "source": [
    "## Sort Merge Join Time Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863967bd-5740-4f61-85f2-6233dbd2cfcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for Sort merge Join: 5.063119649887085 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution time for Sort merge Join: %s seconds\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e575cb0-0175-4b2b-83bf-d308c957302d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  follows.subject follows.object friendOf.object likes.object   \n",
      "0         User454        User964         User104     Product0  \\\n",
      "1         User454        User964         User104     Product0   \n",
      "2         User454        User964         User104     Product0   \n",
      "3         User454        User964         User104     Product0   \n",
      "4         User454        User964         User104     Product0   \n",
      "\n",
      "   hasReview.object  \n",
      "0         Review478  \n",
      "1        Review1458  \n",
      "2        Review1452  \n",
      "3        Review1392  \n",
      "4        Review1344  \n"
     ]
    }
   ],
   "source": [
    "print(sortMergeJoin_QueryResult_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385e98e-16b7-4ce2-a5ba-83ddfe6a7511",
   "metadata": {},
   "source": [
    "## Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effca88f-c555-4fe3-8a4e-775e6700abea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hashJoin(df1, key1, df2, key2):\n",
    "    # Create hash keys for both DataFrames based on the specified keys\n",
    "    df1[\"hash_key\"] = df1[key1].apply(hash)\n",
    "    df2[\"hash_key\"] = df2[key2].apply(hash)\n",
    "    \n",
    "    # Group df1 based on the hash keys to improve join performance\n",
    "    df1Partitions = dict(tuple(df1.groupby('hash_key')))\n",
    "    for key in df1Partitions:\n",
    "        df1Partitions[key].reset_index(drop = True, inplace = True)\n",
    "    \n",
    "     # Get the keys of the grouped DataFrame partitions\n",
    "    dictKeys = df1Partitions.keys()\n",
    "    resultAsList = []\n",
    "    \n",
    "    # Perform the hash join operation\n",
    "    for i in range(len(df2)):\n",
    "        if (df2.iloc[i]['hash_key']) in dictKeys:\n",
    "            key = df2.iloc[i]['hash_key']\n",
    "            joinRow = df2.iloc[[i]]\n",
    "            joinedRow = pd.merge(df1Partitions[key],joinRow, on='hash_key',  how='left')\n",
    "            resultAsList.append(joinedRow.to_dict('list'))\n",
    "    \n",
    "    listOfJoinedPartitions = []\n",
    "    for dictionary in resultAsList:\n",
    "        listOfJoinedPartitions.append(pd.DataFrame(dictionary))\n",
    "    \n",
    "    finalDf = pd.concat(listOfJoinedPartitions)\n",
    "    finalDf = finalDf.drop('hash_key',axis=1)\n",
    "    \n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c8f90b3-e17b-4f38-a022-0de7d7692032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8680/1287216863.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hashJoin_QueryResult_df.rename(columns = {'subject_x_x': 'follows.subject',\n"
     ]
    }
   ],
   "source": [
    "#Hash Join Results\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the hash join operations between 'follows' and 'friendOf' DataFrames\n",
    "# followsObject_friendOfSubject_hashJoin_dataframe\n",
    "followsObject_friendOfSubject_hj_df = hashJoin(\n",
    "    dictionaryDf['follows'], 'object',\n",
    "    dictionaryDf['friendOf'], 'subject')\n",
    "\n",
    "# Perform the hash join operations between 'likes' and 'hasReview' DataFrames\n",
    "#likesObject_hasReviewSubject_ hashJoin Df\n",
    "likesObject_hasReviewSubject_hj_df = hashJoin(\n",
    "    dictionaryDf['likes'], 'object',\n",
    "    dictionaryDf['hasReview'], 'subject')\n",
    "\n",
    "# Perform the hash join operations between the results of previous two joins\n",
    "friendsOfObject_likesSubject_hj_df = hashJoin(\n",
    "    followsObject_friendOfSubject_hj_df, 'object_y',\n",
    "    likesObject_hasReviewSubject_hj_df, 'subject_x')\n",
    "\n",
    "hashJoin_QueryResult_df = friendsOfObject_likesSubject_hj_df[[\"subject_x_x\",\n",
    "                                     \"object_x_x\",\n",
    "                                     \"object_y_x\",\n",
    "                                     \"object_x_y\",\n",
    "                                     \"object_y_y\"]]\n",
    "\n",
    "hashJoin_QueryResult_df.rename(columns = {'subject_x_x': 'follows.subject',\n",
    "                                          'object_x_x': 'follows.object',\n",
    "                                          'object_y_x': 'friendOf.object',\n",
    "                                          'object_x_y': 'likes.object',\n",
    "                                          'object_y_y': 'hasReview.object'}, inplace = True)\n",
    "                               \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271de4d-64a4-46dc-96da-48b2a3579ab0",
   "metadata": {},
   "source": [
    "## Hash Join Time Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a73ee80-4211-465f-97c6-e739acb43960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for Hash Join: 122.14708161354065 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution time for Hash Join: %s seconds\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af89e553-f3c8-4c75-b867-8fc7bfc37576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  follows.subject follows.object friendOf.object likes.object hasReview.object\n",
      "0         User630          User9          User20     Product0         Review24\n",
      "1          User26         User57          User20     Product0         Review24\n",
      "2         User197         User57          User20     Product0         Review24\n",
      "3         User351         User57          User20     Product0         Review24\n",
      "4         User396         User57          User20     Product0         Review24\n"
     ]
    }
   ],
   "source": [
    "print(hashJoin_QueryResult_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d231ef-5821-4870-8ff5-e97998d1d211",
   "metadata": {},
   "source": [
    "## Improved Hash Join "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff74925-d8f1-41d4-ac37-ca3335156762",
   "metadata": {},
   "source": [
    "C) The third task is to design and implement an improvement algorithm regarding the running time.\n",
    "There is no restrictions on the approaches. Possible candidates are: use radix join algorithm, use\n",
    "a different hash function or hashing scheme, partition the data before the join operation, or use\n",
    "parallel sorting algorithms. Other options are for instance building indexes on the data before the\n",
    "query evaluation. Of course you might even invent a completely new join algorithm. In summary\n",
    "everything is allowed as long as the result is correct. Describe your improvement approach in the\n",
    "report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7407cfe-54e8-47e1-8257-c14dc3da7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvedHashJoin(df1, key1, df2, key2):\n",
    "    \n",
    "    df1[\"hash_key\"] = df1[key1].apply(hash)\n",
    "    df2[\"hash_key\"] = df2[key2].apply(hash)\n",
    "    \n",
    "    df1Partitions = dict(tuple(df1.groupby('hash_key')))\n",
    "    for key in df1Partitions:\n",
    "        df1Partitions[key].reset_index(drop = True, inplace = True)\n",
    "\n",
    "    df2Partitions = dict(tuple(df2.groupby('hash_key')))\n",
    "    for key in df2Partitions:\n",
    "        df2Partitions[key].reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    resultAsList = []\n",
    "    for key_2 in df2Partitions:\n",
    "        for key_1 in df1Partitions:\n",
    "            if key_1 == key_2:\n",
    "                joinedPartitions = pd.merge(df1Partitions[key_1],df2Partitions[key_2], on='hash_key',  how='left')\n",
    "                resultAsList.append(joinedPartitions.to_dict('list'))\n",
    "    \n",
    "    listOfJoinedPartitions = []\n",
    "    for dictionary in resultAsList:\n",
    "        listOfJoinedPartitions.append(pd.DataFrame(dictionary))\n",
    "    \n",
    "    finalDf = pd.concat(listOfJoinedPartitions)\n",
    "    finalDf = finalDf.drop('hash_key',axis=1)\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bc3d207-0d31-4658-9226-c26bad5d233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8680/238336458.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  improvedHashJoin_QueryResult_df.rename(columns = {'subject_x_x': 'follows.subject',\n"
     ]
    }
   ],
   "source": [
    "#Improved Hash Join Results\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the improved hash join operations between 'follows' and 'friendOf' DataFrames\n",
    "followsObject_friendOfSubject_ihj_df = improvedHashJoin(\n",
    "    dictionaryDf['follows'], 'object',\n",
    "    dictionaryDf['friendOf'], 'subject')\n",
    "\n",
    "# Perform the improved hash join operations between 'likes' and 'hasReview' DataFrames\n",
    "likesObject_hasReviewSubject_ihj_df = improvedHashJoin(\n",
    "    dictionaryDf['likes'], 'object',\n",
    "    dictionaryDf['hasReview'], 'subject')\n",
    "\n",
    "# Perform the improved hash join operations between the results of the previous two joins\n",
    "friendsOfObject_likesSubject_ihj_df = improvedHashJoin(\n",
    "    followsObject_friendOfSubject_ihj_df, 'object_y',\n",
    "    likesObject_hasReviewSubject_ihj_df, 'subject_x')\n",
    "\n",
    "# Extract only the required columns and rename them for clarity\n",
    "improvedHashJoin_QueryResult_df = friendsOfObject_likesSubject_ihj_df[[\"subject_x_x\",\n",
    "                                     \"object_x_x\",\n",
    "                                     \"object_y_x\",\n",
    "                                     \"object_x_y\",\n",
    "                                     \"object_y_y\"]]\n",
    "\n",
    "improvedHashJoin_QueryResult_df.rename(columns = {'subject_x_x': 'follows.subject',\n",
    "                                          'object_x_x': 'follows.object',\n",
    "                                          'object_y_x': 'friendOf.object',\n",
    "                                          'object_x_y': 'likes.object',\n",
    "                                          'object_y_y': 'hasReview.object'}, inplace = True)\n",
    "                               \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ff0cd-db43-42ae-a6a0-b3045e46960d",
   "metadata": {},
   "source": [
    "## Improved Hash Join Time Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03078489-26d0-4727-9b71-a7f0f5213178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for Improved Hash Join: 49.30752444267273 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Execution time for Improved Hash Join: %s seconds\" %(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2d3dff-37f9-4e21-801e-f1010d99c5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  follows.subject follows.object friendOf.object likes.object hasReview.object\n",
      "0          User75        User250         User602   Product110         Review37\n",
      "1          User75        User250         User602   Product110        Review218\n",
      "2          User75        User250         User602   Product110        Review219\n",
      "3          User75        User250         User602   Product110        Review459\n",
      "4          User75        User250         User602   Product110        Review584\n"
     ]
    }
   ],
   "source": [
    "print(improvedHashJoin_QueryResult_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f79ba-a9ea-426d-bd84-9afcb1affd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
